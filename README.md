# K-Means-Algorithm
This is a set of hand-built, Numpy vectorized algorithms that use a document-term numpy array to create K-Means cluster centroids and cluster assignments. 

These functions will allow you to cluster your documents, then use the model centroids in a classifer algorithm to classify unknown documents.

# Contents: 
- K Means Clustering Algorithm
- K Means Model Classifier
- Random Cluster Centroid Generator
- Various similarity functions

# Code written to address the following challenge:
For this problem you will use a different subset of the [20 Newsgroups data set](http://qwone.com/~jason/20Newsgroups/). This subset includes 2,500 documents (newsgroup posts), each belonging to one of 5 categories windows (0), crypt (1), christian (2), hockey (3), forsale (4). The documents are represented by 9328 terms (stems). The dictionary (vocabulary) for the data set is given in the file `News_group_terms.txt` and the full term­-by-­document matrix is given in `News_group_matrix.txt` (comma separated values). The actual category labels for the documents are provided in the file `News_group_classes.txt`. Your goal is to perform clustering on the documents and compare the clusters to the actual categories. Your tasks in this problem are the following. (You may also use Pandas and other modules from scikit­learn that you may need for preprocessing or evaluation.)
1. Create your own distance function that, instead of using Euclidean distance, uses Cosine similarity. This is the distance function you will use to pass to the kMeans function.
2. Load the data set [Note: the data matrix provided has terms as rows and documents as columns. Since you will be clustering documents, you'll need to take the transpose of this matrix so that your main data matrix is a document x term matrix. In Numpy, you may use the ".T" operation to obtain the transpose.] Then, split the data set (the document x term matrix) and set aside 20% for later use (see below). Use the 80% segment for clustering in the next part. The 20% portion must be a random subset.
3. Transform the term-­frequencies to TFxIDF values. Be sure to maintain DF values for each of the terms in the dictionary. [Note: if you run into problems due to limited computational resources, you may prune the data by removing all terms with low DF values, e.g., terms that appear in less than 10 documents. Be sure to maintain the correspondence between the dictionary terms and the
matrix rows.]
4. Perform Kmeans clustering on the training data. Write a function to display the top N terms in each cluster along with the cluster DF values for each term and the size of the cluster. Cluster DF value for a term t in a cluster C is the percentage of docs in cluster C in which term t appears. Sort the terms in decreasing order of the DF percentage. Here is an example of how this output might look like (here the top 10 terms for 3 of the 5 clusters are displayed in decreasing order of cluster DF values, but the mean frequnecy from the cluster centroid is also shown). [Bonus points: use your favorite third party tool, ideally with a Python based API, to create a word cloud for each cluster based on the in­cluster DF values.]
5. Using the cluster assignments from Kmeans clustering, compare your 5 clusters to the 5 pre­assigned classes by computing the Completeness and Homogeneity values.
6. Finally, using your cluster assignments as class labels, categorize each of the documents in the 20% setaside data into each of the appropriate cluster. Your categorization should be based on  cosine similarity between each test document and each cluster centroids. Present your results in a separate file containing the obtained cluster label for each test document as well as Cosine similarities between each test document and each of the 5 clusters.